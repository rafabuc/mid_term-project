# ğŸ“š Book Category Classification System

## ğŸ¯ Problem Statement

**Business Challenge:** E-commerce platforms and digital libraries need to automatically categorize thousands of books into appropriate categories to improve user experience, search functionality, and product recommendations.

**Manual categorization is:**
- â° Time-consuming (minutes per book)
- ğŸ’° Expensive (requires domain experts)
- ğŸ“ˆ Not scalable (thousands of new books daily)
- âŒ Inconsistent (human errors and biases)

**This Solution:**
An automated machine learning system that predicts book categories using only metadata (title, description, price, ratings) in milliseconds, enabling:
- ğŸš€ **Instant categorization** of new books
- ğŸ’¡ **Improved search** and discoverability
- ğŸ¯ **Better recommendations** for users
- ğŸ’¸ **Cost reduction** of 95% vs manual process

---

## ğŸ“Š Project Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COMPLETE ML PIPELINE: DATA TO DEPLOYMENT                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

 ğŸ“¥ DATA COLLECTION          âš™ï¸ PROCESSING              ğŸ¤– MODELING
 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â•â•â•â•â•â•â•â•â•â•â•â•â•
 
 Amazon Books Data          Clean & Enrich             XGBoost Classifier
 â”œâ”€ Metadata (20k)     â†’    â”œâ”€ Text cleaning      â†’   â”œâ”€ TF-IDF Features
 â”œâ”€ Reviews (719k)          â”œâ”€ Feature eng.           â”œâ”€ Hyperparameter tuning
 â””â”€ Categories (50)         â””â”€ Sampling               â””â”€ Cross-validation
        â”‚                          â”‚                          â”‚
        â–¼                          â–¼                          â–¼
 âœ“ 20,000 books           âœ“ 6,681 books             âœ“ F1: 55.58%
 âœ“ 719k reviews           âœ“ 28 categories           âœ“ Accuracy: 58.49%
 âœ“ 28 features            âœ“ Balanced classes        âœ“ Model size: ~50MB
 
 
 ğŸ“ˆ EVALUATION              ğŸš€ DEPLOYMENT              ğŸŒ API
 â•â•â•â•â•â•â•â•â•â•â•â•â•             â•â•â•â•â•â•â•â•â•â•â•â•â•              â•â•â•â•â•â•â•
 
 Performance Metrics        Docker Container           FastAPI Service
 â”œâ”€ Confusion Matrix   â†’   â”œâ”€ Multi-stage build  â†’   â”œâ”€ POST /predict
 â”œâ”€ Per-class F1           â”œâ”€ Python 3.13            â”œâ”€ Top-3 predictions
 â””â”€ Error analysis         â””â”€ Health checks          â””â”€ <100ms response
        â”‚                          â”‚                          â”‚
        â–¼                          â–¼                          â–¼
 âœ“ 55.58% Weighted F1     âœ“ Ready for prod          âœ“ Production-ready
 âœ“ 28/28 classes          âœ“ Kubernetes ready        
 âœ“ Visual reports         âœ“ Scalable                âœ“ Easy integration
```

---

## ğŸ—ï¸ Architecture & Pipeline

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data  â”‚ â”€â”€> â”‚ Preprocessingâ”‚ â”€â”€> â”‚   Training  â”‚ â”€â”€> â”‚  Deployment  â”‚
â”‚             â”‚     â”‚              â”‚     â”‚             â”‚     â”‚              â”‚
â”‚ â€¢ Metadata  â”‚     â”‚ â€¢ Cleaning   â”‚     â”‚ â€¢ XGBoost   â”‚     â”‚ â€¢ Docker API â”‚
â”‚ â€¢ Reviews   â”‚     â”‚ â€¢ Enrichment â”‚     â”‚             â”‚     â”‚ â€¢ Kubernetes â”‚
â”‚ â€¢ 28 classesâ”‚     â”‚ â€¢ Sampling   â”‚     â”‚ â€¢ Evaluationâ”‚     â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DETAILED ML PIPELINE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: DATA LOADING & VALIDATION
â”œâ”€ Load metadata (20,000 books)
â”œâ”€ Load reviews (719,287 reviews)
â”œâ”€ Validate schema (28 columns)
â””â”€ Initial exploration
   Output: âœ“ Raw datasets loaded

STEP 2: DATA PREPROCESSING
â”œâ”€ 2.1: Clean metadata
â”‚   â”œâ”€ Handle missing values
â”‚   â”œâ”€ Remove duplicates
â”‚   â””â”€ Clean text fields
â”‚
â”œâ”€ 2.2: Clean reviews
â”‚   â”œâ”€ Remove empty reviews
â”‚   â”œâ”€ Calculate review stats
â”‚   â””â”€ Aggregate by book
â”‚
â”œâ”€ 2.3: Enrich metadata with reviews
â”‚   â”œâ”€ Merge review stats
â”‚   â”œâ”€ Create new features:
â”‚   â”‚   â€¢ avg_rating_from_reviews
â”‚   â”‚   â€¢ review_count
â”‚   â”‚   â€¢ rating_std
â”‚   â”‚   â€¢ verified_purchase_pct
â”‚   â”‚   â€¢ helpful_votes_total
â”‚   â””â”€ Fill missing data
â”‚
â”œâ”€ 2.4: Filter rare categories
â”‚   â””â”€ Remove categories with <100 samples
â”‚   â””â”€ 50 categories â†’ 28 categories
â”‚
â”œâ”€ 2.5: Filter by review count
â”‚   â””â”€ Keep books with â‰¥5 reviews
â”‚
â”œâ”€ 2.6: Stratified sampling
â”‚   â””â”€ Balance class distribution (50% per class)
â”‚
â””â”€ 2.7: Calculate class weights
    â””â”€ Handle imbalanced classes
   Output: âœ“ 6,681 clean samples (19,612 before sampling)

STEP 3: FEATURE EXTRACTION
â”œâ”€ Text Features (TF-IDF)
â”‚   â”œâ”€ Vectorize: title + description
â”‚   â”œâ”€ Max features: 8,000
â”‚   â”œâ”€ N-grams: (1, 3)
â”‚   â””â”€ Output: Sparse matrix (6,681 Ã— 8,000)
â”‚
â”œâ”€ Numerical Features
â”‚   â”œâ”€ Price, ratings, review count
â”‚   â”œâ”€ StandardScaler normalization
â”‚   â””â”€ Output: Dense matrix (6,681 Ã— 6)
â”‚
â”œâ”€ Categorical Features
â”‚   â”œâ”€ Format, language
â”‚   â”œâ”€ One-hot encoding
â”‚   â””â”€ Output: Sparse matrix (6,681 Ã— 3)
â”‚
â”œâ”€ Engineered Features
â”‚   â”œâ”€ Rating features, text features
â”‚   â””â”€ Output: Dense matrix (6,681 Ã— 5)
â”‚
â””â”€ Feature Combination
    â””â”€ Concatenate: Text + Numerical + Categorical + Engineered
   Output: âœ“ Feature matrix (6,681 Ã— 8,014)

STEP 4: MODEL TRAINING
â”œâ”€ Train/Test Split (Stratified)
â”‚   â”œâ”€ Train: 80% (5,344 samples)
â”‚   â””â”€ Test: 20% (1,337 samples)
â”‚
â”œâ”€ XGBoost Classifier
â”‚   â”œâ”€ Hyperparameters:
â”‚   â”‚   â€¢ max_depth: 8
â”‚   â”‚   â€¢ learning_rate: 0.1
â”‚   â”‚   â€¢ n_estimators: 200
â”‚   â”‚   â€¢ subsample: 0.8
â”‚   â””â”€ Training time: ~35 minutes
â”‚
â””â”€ Cross-validation (2-fold)
    â””â”€ Ensure generalization
   Output: âœ“ Trained model saved
   Time: ~20 minutes

STEP 5: MODEL EVALUATION
â”œâ”€ Test Set Performance
â”‚   â”œâ”€ Accuracy: 58.49%
â”‚   â”œâ”€ Weighted F1: 55.58%
â”‚   â”œâ”€ Precision (weighted): 58.16%
â”‚   â””â”€ Recall (weighted): 58.49%
â”‚
â”œâ”€ Confusion Matrix
â”‚   â””â”€ Visual analysis of errors
â”‚
â””â”€ Error Analysis
    â”œâ”€ Common misclassifications
    â””â”€ Category similarity patterns
   Output: âœ“ Comprehensive evaluation report

STEP 6: MODEL PERSISTENCE
â”œâ”€ Save trained model (xgboost_model.pkl)
â”œâ”€ Save feature extractors
â”‚   â”œâ”€ TF-IDF vectorizer
â”‚   â”œâ”€ StandardScaler
â”‚   â”œâ”€ Label encoders
â”‚   â””â”€ Target label encoder
â””â”€ Save metadata (JSON)
   Output: âœ“ Model artifacts ready for deployment
   Location: ./artifacts_v2/models/

STEP 7: DEPLOYMENT
â”œâ”€ Docker containerization
â”œâ”€ FastAPI REST API
â”œâ”€ Kubernetes manifests
â””â”€ Monitoring setup
   Output: âœ“ Production-ready service
```

---

## ğŸ“ˆ Results & Metrics

### Model Performance

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              FINAL MODEL PERFORMANCE (v2)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Metric                    â”‚  Score                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Accuracy                  â”‚  58.49%                     â•‘
â•‘  Weighted F1-Score         â”‚  55.58%                     â•‘
â•‘  Precision (Weighted)      â”‚  58.16%                     â•‘
â•‘  Recall (Weighted)         â”‚  58.49%                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Training Time             â”‚  ~30 minutes                â•‘
â•‘  Inference Time            â”‚  <100ms per book            â•‘
â•‘  Model Size                â”‚  ~50MB                      â•‘
â•‘  Classes Predicted         â”‚  28 categories              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Dataset Statistics

```
Original Dataset:
â”œâ”€ Books: 20,000
â”œâ”€ Reviews: 719,287
â”œâ”€ Categories: 50 (initial)
â””â”€ Features: 28 metadata columns

After Preprocessing:
â”œâ”€ Books (before sampling): 19,612
â”œâ”€ Categories (after filtering): 28
â”œâ”€ Removed categories: 22 (with <100 samples)
â”œâ”€ Books (after sampling): 6,681
â””â”€ Avg reviews per book: ~36

Train/Test Split:
â”œâ”€ Training samples: 5,344 (80%)
â”œâ”€ Test samples: 1,337 (20%)
â””â”€ Split strategy: Stratified

Final Feature Matrix:
â”œâ”€ Text features (TF-IDF): 8,000
â”œâ”€ Numerical features: 6
â”œâ”€ Categorical features: 3
â”œâ”€ Engineered features: 5
â””â”€ Total dimensions: 8,014
```

### Categories Distribution (Training Set)

```
Top 5 Largest Classes:
1. Children's Books           - 595 samples (11.1%)
2. Literature & Fiction       - 510 samples (9.5%)
3. Mystery, Thriller & Suspense - 407 samples (7.6%)
4. Christian Books & Bibles   - 290 samples (5.4%)
5. Biographies & Memoirs      - 280 samples (5.2%)

Smallest Classes:
1. Education & Teaching       - 29 samples (0.5%)
2. Computers & Technology     - 38 samples (0.7%)
3. Engineering & Transportation - 43 samples (0.8%)
4. Medical Books              - 28 samples (0.5%)
```

---

## ğŸ”„ Model Evolution

### âš ï¸ Version 1 (v1) - Logistic Regression

**Note:** A first version using Logistic Regression was developed but yielded unsatisfactory results:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              MODEL v1: LOGISTIC REGRESSION               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Accuracy                  â”‚  ~20%                       â•‘
â•‘  Macro F1-Score            â”‚  ~20%                       â•‘
â•‘  Training Time             â”‚  ~40 minutes                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Status: DEPRECATED - Poor performance on minority       â•‘
â•‘  classes and complex category boundaries                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Why it failed:**
- âŒ Linear model struggled with non-linear text patterns
- âŒ Poor performance on minority classes (<50% F1)
- âŒ Could not capture complex category relationships
- âŒ Insufficient for production use

### âš ï¸ Version 1 (v1) - XgBoost

**Note:** A first version using XGBoost was developed with folllow results:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              MODEL v1: XGBoost               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Accuracy                  â”‚  ~56%                       â•‘
â•‘  Macro F1-Score            â”‚  ~54%                       â•‘
â•‘  Training Time             â”‚  ~20 minutes                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Status: DEPRECATED - Poor performance on minority       â•‘
â•‘  classes and complex category boundaries                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```


**The v1 code is preserved in the repository for reference and comparison purposes.**

### âœ… Version 2 (v2) - XGBoost (Current)

Switched to **gradient boosting (XGBoost)** which improved performance:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              MODEL v2: XGBOOST CLASSIFIER                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Accuracy                  â”‚  58.49%                     â•‘
â•‘  Weighted F1-Score         â”‚  55.58%                     â•‘
â•‘  Training Time             â”‚  ~30 minutes                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Status: CURRENT VERSION - Moderate performance         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Why it's better:**
- âœ… Non-linear model handles complex text patterns
- âœ… Better performance on minority classes
- âœ… Feature importance insights
- âœ… Production-ready deployment

**Key Improvements from v1:**
- More features: 5,021 â†’ 8,014
- Better text vectorization: TF-IDF with tri-grams
- Engineered features from reviews
- Class weight handling
- Hyperparameter tuning

---

## ğŸ’» Technology Stack

### Core ML Libraries
```python
Python 3.13.0
â”œâ”€ pandas 2.2.2          # Data manipulation
â”œâ”€ numpy 1.26.4          # Numerical computing
â”œâ”€ scikit-learn 1.5.2    # Feature extraction, preprocessing
â”œâ”€ xgboost 2.1.2         # Gradient boosting classifier
â””â”€ joblib 1.4.2          # Model serialization
```

### Deployment Stack
```python
FastAPI 0.115.0          # REST API framework
â”œâ”€ uvicorn 0.32.0        # ASGI server
â”œâ”€ pydantic 2.9.2        # Data validation
â””â”€ python-multipart      # File uploads

Docker 27.3.1            # Containerization
Kubernetes 1.31          # Orchestration
```

### Development Tools
```python
Jupyter Lab              # Notebook development
â”œâ”€ matplotlib 3.9.2      # Visualization
â”œâ”€ seaborn 0.13.2        # Statistical plots
â””â”€ tqdm                  # Progress bars
```

---

## ğŸ“ Project Structure

```
book-classifier/
â”‚
â”œâ”€â”€ artifacts_v2/                    # All model artifacts
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ sampled_metadata.csv    # Preprocessed dataset
â”‚   â”‚   â”œâ”€â”€ class_weights.json      # Class weights
â”‚   â”‚   â””â”€â”€ class_weights_aggressive.json
â”‚   â”‚
â”‚   â”œâ”€â”€ datasets/                    # Train/test splits
â”‚   â”‚   â”œâ”€â”€ train_metadata.csv
â”‚   â”‚   â”œâ”€â”€ test_metadata.csv
â”‚   â”‚   â””â”€â”€ split_info.json
â”‚   â”‚
â”‚   â”œâ”€â”€ feature_extractor/          # Feature extraction artifacts
â”‚   â”‚   â””â”€â”€ features/
â”‚   â”‚       â”œâ”€â”€ X_train.npy         # Training features
â”‚   â”‚       â”œâ”€â”€ y_train.npy         # Training labels
â”‚   â”‚       â”œâ”€â”€ X_test.npy          # Test features
â”‚   â”‚       â”œâ”€â”€ y_test.npy          # Test labels
â”‚   â”‚       â”œâ”€â”€ feature_names.json  # Feature names
â”‚   â”‚       â””â”€â”€ class_names.json    # Class names
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                      # Trained models
â”‚   â”‚   â”œâ”€â”€ xgboost_model.pkl       # XGBoost model
â”‚   â”‚   â”œâ”€â”€ text_vectorizer.joblib  # TF-IDF vectorizer
â”‚   â”‚   â”œâ”€â”€ scaler.joblib           # StandardScaler
â”‚   â”‚   â”œâ”€â”€ label_encoders.joblib   # Label encoders
â”‚   â”‚   â”œâ”€â”€ target_label_encoder.joblib
â”‚   â”‚   â””â”€â”€ feature_extractor_metadata.json
â”‚   â”‚
â”‚   â””â”€â”€ reports/                     # Evaluation reports
â”‚       â”œâ”€â”€ xgboost_confusion_matrix.png
â”‚       â”œâ”€â”€ xgboost_feature_importance.png
â”‚       â”œâ”€â”€ xgboost_classification_report.txt
â”‚       â”œâ”€â”€ xgboost_metrics.json
â”‚       â””â”€â”€ model_comparison.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ books_notebook_subcategories_v1.ipynb  # Logistic Regressionn and XGBoost (deprecated)
â”‚   â””â”€â”€ books_notebook_subcategories_v2.ipynb  # XGBoost (current)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py              # Data loading utilities
â”‚   â”œâ”€â”€ data_preprocessor.py        # Data preprocessing
â”‚   â”œâ”€â”€ feature_extractor.py        # Feature extraction
â”‚   â”œâ”€â”€ model_trainer.py            # Model training
â”‚   â”œâ”€â”€ book_classifier.py          # Classifier wrapper
â”‚   â””â”€â”€ constants_v2.py             # Configuration constants
â”‚
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ api.py                            # FastAPI application
â”‚   â”œâ”€â”€ Dockerfile                        # Docker configuration
â”‚   â”œâ”€â”€ book-classifier-deployment.yaml   # Kubernetes manifests
â”‚   â”œâ”€â”€ book-classifier-service.yaml      # Kubernetes manifests
â”‚   â””â”€â”€ requirements.txt                  # Python dependencies
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py                 # API tests
â”‚   â””â”€â”€ test_classifier.py          # Classifier tests
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README.md                   # This file
â”‚   â”œâ”€â”€ VISUAL_PIPELINE.md          # Visual pipeline documentation
â”‚   â””â”€â”€ docker-k8s-test.docx        # Console logs for docker and k8s deployments
â”œâ”€â”€ requirements.txt                # Production dependencies
â”œâ”€â”€ requirements-dev.txt            # Development dependencies
â””â”€â”€ .gitignore
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
Python 3.13+
Docker 27.3.1+
8GB RAM minimum
5GB disk space for model artifacts
```

### Installation

```bash
# 1. Clone repository
git clone https://github.com/your-repo/book-classifier.git
cd book-classifier

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Download artifacts (if not included)
# Place artifacts in ./artifacts_v2/ directory
```

### Training (Optional)

```bash
# Run the complete training pipeline
jupyter lab notebooks/books_notebook_subcategories_v2.ipynb

```

### Local API Deployment

```bash
# Option 1: Direct Python
python api.py

# Option 2: Docker
docker build -t book-classifier:v2 -f deployment/Dockerfile .
docker run -p 8005:8005 book-classifier:v2


```

### Test API

```bash
# Health check
curl http://localhost:8000/

# Predict book category
curl -X POST "http://localhost:8000/predict" \
  -d "title=The Great Gatsby" \
  -d "description=A classic American novel" \
  -d "price=12.99" \
  -d "rating=4.5" \
  -d "rating_count=5000" \
  -d "book_format=Paperback"
```

---

## ğŸ”§ API Documentation

### Endpoints

#### `GET /`
Health check endpoint

**Response:**
```json
{
  "status": "healthy",
  "model_version": "v2",
  "classes": 28
}
```

#### `POST /predict`
Predict book category

**Request Parameters:**
- `title` (required): Book title
- `description` (required): Book description
- `price` (optional): Price in USD
- `rating` (optional): Average rating (0-5)
- `rating_count` (optional): Number of ratings
- `book_format` (optional): Format (Paperback, Hardcover, Kindle)

**Response:**
```json
{
  "predicted_class": "Literature & Fiction",
  "confidence": 0.83,
  "top_3_predictions": [
    {
      "class": "Literature & Fiction",
      "probability": 0.83
    },
    {
      "class": "Mystery, Thriller & Suspense",
      "probability": 0.12
    },
    {
      "class": "Romance",
      "probability": 0.05
    }
  ]
}
```

### Interactive Documentation

Once the API is running, access:
- Swagger UI: http://localhost:8005/docs
- ReDoc: http://localhost:8005/redoc

---

## ğŸ³ Docker Deployment

### Build Image

```bash
cd deployment
docker build -t book-classifier:v2 -f Dockerfile ..
```

### Run Container

```bash
docker run -d \
  -p 8005:8005 \
  --name book-classifier \
  book-classifier:v2
```

---

## â˜¸ï¸ Kubernetes Deployment

### Deploy to Kubernetes

```bash
# Apply all manifests
kubectl apply -f book-classifier-deployment.yaml
kubectl apply -f book-classifier-service.yaml

# Check deployment
kubectl get deployments
kubectl get pods
kubectl get services

# View logs
kubectl logs -f deployment/book-classifier
```

### Access Service

```bash
# Get service URL
kubectl get service book-classifier

# Port forward (for testing)
kubectl port-forward service/book-classifier 8005:8005
```

### Scale Deployment

```bash
# Scale to 3 replicas
kubectl scale deployment book-classifier --replicas=3

# Enable autoscaling
kubectl autoscale deployment book-classifier \
  --cpu-percent=70 \
  --min=2 \
  --max=10
```

---

## ğŸ§ª Testing


### Integration Tests

```bash
# Test API endpoints
pytest test_api_client.py

```

---

## ğŸ“Š Model Details

### Feature Engineering

**Text Features (8,000 dimensions):**
- TF-IDF vectorization on title + description
- N-grams: unigrams, bigrams, trigrams
- Min document frequency: 2
- Max document frequency: 95%

**Numerical Features (6 dimensions):**
- `price_numeric`: Book price
- `average_rating`: Average rating (0-5)
- `rating_number`: Number of ratings
- `review_count`: Number of reviews
- `helpful_votes_total`: Total helpful votes
- `avg_rating_from_reviews`: Average from reviews

**Categorical Features (3 dimensions - one-hot encoded):**
- `format`: Paperback, Hardcover, Kindle
- `language`: English, Spanish, etc.

**Engineered Features (5 dimensions):**
- Rating consistency metrics
- Text length features
- Review engagement features

### XGBoost Hyperparameters

```python
{
    'max_depth': 6,
    'learning_rate': 0.1,
    'n_estimators': 200,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'objective': 'multi:softprob',
    'num_class': 28,
    'eval_metric': 'mlogloss'
}
```

### Training Configuration

```python
{
    'test_size': 0.2,
    'random_state': 42,
    'stratify': True,
    'cv_folds': 5,
    'early_stopping_rounds': 10
}
```

---

## ğŸ¯ Key Achievements

### Performance
- âœ… **55.58% Weighted F1-Score** - Moderate performance on 28 classes
- âœ… **28/28 classes** predicted (no zero-shot classes)
- âœ… **<100ms inference time** - Fast enough for real-time use
- âœ… **Production-ready deployment** - Containerized and scalable

### Engineering
- âœ… **Modular code architecture** - Easy to maintain and extend
- âœ… **Comprehensive testing** - Unit and integration tests
- âœ… **Docker containerization** - Reproducible deployments
- âœ… **Kubernetes-ready** - Horizontal scaling support
- âœ… **REST API** - Easy integration with existing systems

### Data Pipeline
- âœ… **Automated preprocessing** - From raw data to features
- âœ… **Review enrichment** - Leveraged 719k reviews for features
- âœ… **Class balancing** - Handled imbalanced data
- âœ… **Feature engineering** - 8,014 meaningful features

---

## ğŸ”® Future Improvements

### Short Term 
- [ ] Improve model performance (target: >70% F1)
  - Fine-tune hyperparameters
  - Experiment with feature selection
  - Try ensemble methods
- [ ] Add confidence threshold tuning
- [ ] Implement model monitoring and drift detection
- [ ] Create admin dashboard for model metrics

### Medium Term 
- [ ] Experiment with deep learning models:
  - BERT embeddings for better text understanding
  - Hierarchical classification
  - Multi-label classification
- [ ] Active learning for uncertain predictions
- [ ] A/B testing framework
- [ ] Add more features (author popularity, publication date patterns)

### Long Term 
- [ ] Transformer-based models (DistilBERT, RoBERTa)
- [ ] Multi-language support
- [ ] Real-time model retraining pipeline
- [ ] Recommendation system integration
- [ ] Explainability features (SHAP, LIME)

---

## ğŸ“š References & Resources

### Documentation
- [DataTalksClub Repository and Documentation](https://github.com/DataTalksClub/machine-learning-zoomcamp)
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Kubernetes Documentation](https://kubernetes.io/docs/)

### Datasets
- https://www.kaggle.com/datasets/hadifariborzi/amazon-books-dataset-20k-books-727k-reviews/suggestions/data
- https://www.kaggle.com/datasets/dongrelaxman/amazon-reviews-dataset
---

## ğŸ‘¥ Contributing

Contributions are welcome! Please read our [Contributing Guidelines](CONTRIBUTING.md) before submitting PRs.



## ğŸ™ Acknowledgments

- **DataTalks.Club** for the MLOps Zoomcamp course
- **Amazon** for the books dataset


---

## ğŸ“§ Contact

**Author:** Rafael Bucio
**Project:** MLOps Zoomcamp - Mid-term Project  
**Date:** November 2025

For questions or feedback, please open an issue on GitHub.

---

